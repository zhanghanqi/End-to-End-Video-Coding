# Video Compression through Image Interpolation - ECCV2018
> [代码](https://github.com/chaoyuaw/pytorch-vci)

> 创新点：

通过**重复图片插值**对视频进行压缩。传统视频帧分为I、B、P帧，这篇文献将视频帧分为I帧和R帧。I帧，也就是关键帧，可以选用任何的图片压缩技术。文章设计了R帧的插值方法，**通过分层插值编解码R帧**。

<img src="https://i.imgur.com/nvIDT7z.png" >

> R帧插值流程

- 每12帧作为一个GOP，第一帧作为关键帧$I_0$
- 相邻两个GOP的关键帧$I_0$和$I_{12}$首先完成中间$I_6$第一次插值
- 然后第一层插值获取的帧$I_6$再分别和左右关键帧$I_0$和$I_{12}$完成第二层插得到$I_3$和$I_{9}$
- 第二层插值获取的帧$I_3$再与关键帧$I_0$完成第三层插值得到$I_1$和$I_{2}$，$I_3$再与第一层得到的帧$I_6$完成第三层插值得到$I_4$和$I_5$

**三层分层插值，需要训练三个插值模型**

> 网络结构

- 网络结构一：encoder（编码）使用Conv-LSTM网络，融合了U-net网络提取的时空上下文信息
- 网络结构二：Binarizer（量化）使用一个1*1的卷积网络与一些非线性操作，本文将特征量化成-1和1两个值
- 网络结构三：decoder （解码）使用Conv-LSTM网络，融合了U-net网络提取的时空上下文信息
- 网络结构四：Entropy coding（熵编码）的核心在于对二进制数的概率估计，本文采用Pixel-CNN进行概率估计

> 编解码器数据流动过程

**上下文帧（context frame）是待插值帧的前一帧与后一帧，比如上面例子中的第一层插值中，$I_6$的context frame就是$I_0$和$I_{12}$。**

- 待压缩帧的前一帧（context fame）和后一帧（context fame）首先用U-net进行特征提取
- 利用运动信息对context frame的U-net特征进行warp操作（其实就是pytorch的grid_sample操作）
- 将warp操作后的特征与编码器网络（Conv-LSTM网络）的中间特征进行通道融合，进行后续的编码工作
- 本文有三个插值模型，所以这里也对应有三种特种融合方式

> 自制数据

- 作者提供的样例图片的格式，一个GOP是12帧，需要把视频分解成包含12帧的视频片段。每张图为有四张运动估计图，因为每一帧待插值图片的context frame有两帧图片，而且运动估计有x、y方向。

- 运动估计算法可以选取任何运动估计算法。

> 结论

总的来说，本文并没有提出新的深度网络结构，只是组合不同的网络用于视频编解码。作者的实验表明，效果与H264相当，要差于H265。工业界用的x265算法无论从编解码效率还是编解码效果上看，都要远远好于H265。而且，这篇文献用的网络包含RNN，不利于网络进行芯片化。这篇文献只是提供了思路，离真正落地应用还有相当长的路要走。
